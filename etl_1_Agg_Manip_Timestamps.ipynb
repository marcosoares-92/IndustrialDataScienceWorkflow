{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c6e02507-dc59-4816-9188-af68e1a6147c"
   },
   "source": [
    "# **Aggregation of Dataframes and Manipulation of Timestamps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0197ceef-0fc0-497f-91d6-8e63fb31ecdd"
   },
   "source": [
    "## _ETL Workflow Notebook 1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d67a6f2a-67b9-4d66-bab4-f00909e32505"
   },
   "source": [
    "## Content:\n",
    "1. Applying a list of row filters;\n",
    "2. Merging on timestamp;\n",
    "3. Merging (joining) dataframes on given keys; and sorting the merged table;\n",
    "4. Record linkage: fuzzy merging (joining) of dataframes on similar strings;\n",
    "5. Concatenating (SQL Union/Stacking/Appending) dataframes;\n",
    "6. Dataframe general characterization;\n",
    "7. Dropping specific columns or rows from the dataframe;\n",
    "8. Removing duplicate rows from the dataframe;\n",
    "9. Removing all columns and rows that contain only missing values;\n",
    "10. Grouping by timestamp;\n",
    "11. Grouping by a given variable;\n",
    "12. Extracting timestamp information;\n",
    "13. Calculating differences between successive timestamps (delays);\n",
    "14. Calculating timedeltas;\n",
    "15. Adding or subtracting timedeltas;\n",
    "16. Slicing the dataframe (selecting a specific subset of rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "6083b7e8-5cde-45d6-a46d-78914d0937df",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import idsw\n",
    "from idsw import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "40b79c11-c21c-4704-9038-49f3f10b46fe",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "69d19ebb-6f82-4dc5-a44d-9f7f5e93fe9a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e1189468-ca9d-4808-bb47-2e063c7011ee",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7ed4a4b0-7cae-4fd7-855b-0b938714d4dd",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "16513167-f7ff-42c6-bd3f-301177ad701c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b32593e4-055e-49d4-a9bb-08de8d9af956",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1ba06e9d-ac6a-4c9f-8c0a-eba4e6ce5938"
   },
   "source": [
    "### **Applying a list of row filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "394305c6-a6c8-48a9-8e10-6ae5efd97e26",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = etl.APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "63aaa1c8-970a-4bf3-b028-5eb2b326e38c"
   },
   "source": [
    "### **Merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d049d4d2-66df-46fb-bb2d-302a32d8c7e7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"DATE\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"DATE\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"outer\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\". This option has no effect \n",
    "# if MERGE_METHOD = \"asof\". Keep inside quotes.\n",
    "\n",
    "MERGE_METHOD = \"ordered\"\n",
    "# Alternatively: MERGE_METHOD = 'ordered' to use pandas .merge_ordered method, or\n",
    "# MERGE_METHOD = \"asof\" for using the .merge_asof method.\n",
    "# WARNING: .merge_asof uses fuzzy matching, so the HOW_TO_JOIN parameter is not applicable.\n",
    "# Keep inside quotes.\n",
    "\n",
    "## USE MERGE_METHOD = 'asof' to merge data collected asynchronously, i.e., data collected in\n",
    "# different moments, resulting in timestamps that do not perfectly match.\n",
    "# merge_asof method sorts the timestamps in ascending order and does not look for a perfect \n",
    "# combination of keys. Instead, it takes the timestamp from the right dataframe as key, and \n",
    "# searches for the closest dataframe on the left dataframe. So, it inputs the row from the right on \n",
    "# the correct position it should have in the left dataframe (in other words, it appends the rows from\n",
    "# one into the order, respecting the columns and the time order).\n",
    "# If a missing value would be generated, the 'ffill' parameter can be used to automatically \n",
    "# repeat the previous value (from the left dataframe) on the row that came from the right table,\n",
    "# filling the missing values.\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "ASOF_DIRECTION = \"nearest\"\n",
    "# Parameter of .merge_asof method. 'nearest' merge the closest timestamps in both directions.\n",
    "# Alternatively: 'backward' or 'forward'.\n",
    "# This option has no effect if MERGE_METHOD = \"ordered\". Keep inside quotes.\n",
    "\n",
    "ORDERED_FILLING = 'ffill'\n",
    "# Parameter or .merge_ordered method.\n",
    "# Alternatively: ORDERED_FILLING = 'ffill' (inside quotes) to fill missings \n",
    "# with the previous value.\n",
    "# This option has no effect if MERGE_METHOD = \"asof\", so you can keep it None\n",
    "\n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = etl.MERGE_ON_TIMESTAMP (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merge_method = MERGE_METHOD, merged_suffixes = MERGED_SUFFIXES, asof_direction = ASOF_DIRECTION, ordered_filling = ORDERED_FILLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "37cb8679-2548-40ca-a387-8a24909bbb88"
   },
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9cff905e-7c3f-44bb-997e-df02aba6aa36",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = etl.MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "991f42df-ee3f-4cae-82b9-d79c35bb5fc9"
   },
   "source": [
    "### **Record linkage: fuzzy merging (joining) of dataframes on similar strings**\n",
    "- Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4dcc3fd4-096f-42cc-b837-27b3f7934dd1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Record linkage is the act of linking data from different sources regarding the same entity.\n",
    "# Generally, we clean two or more DataFrames, generate pairs of potentially matching records, \n",
    "# score these pairs according to string similarity and other similarity metrics, and link them. \n",
    "# Example: we may want to merge data from different clients using the address as key, but there may\n",
    "# be differences on the format used for registering the addresses.\n",
    "\n",
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "COLUMNS_TO_BLOCK_AS_BASIS_FOR_COMPARISON = {'left_df_column': None, \n",
    "                                            'right_df_column': None}\n",
    "# COLUMNS_TO_BLOCK_AS_BASIS_FOR_COMPARISON = = {'left_df_column': None, 'right_df_column': None}\n",
    "# Dictionary of strings, in quotes. Do not change the keys. If a pair of columns should be\n",
    "# blocked for being used as basis for merging declare here: in 'left_df_column', input the name\n",
    "# of the column of the left dataframe. In right_df_column, input the name of the column on the right\n",
    "# dataframe.\n",
    "# We first want to generate pairs between both DataFrames. Ideally, we want to generate all\n",
    "# possible pairs between our DataFrames.\n",
    "# But, if we had big DataFrames, it is possible that we ende up having to generate millions, \n",
    "# if not billions of pairs. It would not prove scalable and could seriously hamper development time.  \n",
    "# This is where we apply what we call blocking, which creates pairs based on a matching column, \n",
    "# reducing the number of possible pairs.\n",
    "\n",
    "THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0\n",
    "# THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0 - 0.0% means no similarity and 100% means equal strings.\n",
    "# The THRESHOLD_FOR_PERCENT_OF_SIMILARITY is the minimum similarity calculated from the\n",
    "# Levenshtein (minimum edit) distance algorithm. This distance represents the minimum number of\n",
    "# insertion, substitution or deletion of characters operations that are needed for making two\n",
    "# strings equal.\n",
    "\n",
    "COLUMNS_WHERE_EXACT_MATCHES_ARE_REQUIRED = [\n",
    "    \n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None}\n",
    "    \n",
    "]\n",
    "\n",
    "COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND = [\n",
    "    \n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None},\n",
    "    {'left_df_column': None, 'right_df_column': None}\n",
    "    \n",
    "]\n",
    "# Both of these arguments have the same structure. The single difference is that\n",
    "# COLUMNS_WHERE_EXACT_MATCHES_ARE_REQUIRED is referent to a group of columns (or a single column)\n",
    "# where we require perfect correspondence between the dataframes, i.e., where no differences are\n",
    "# tolerated. Example: the month and day numbers must be precisely the same.\n",
    "# COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND, in turns, is referent to the columns where there\n",
    "# is no rigid standard in the dataset, so similar values should be merged as long as the similarity\n",
    "# is equal or higher than THRESHOLD_FOR_PERCENT_OF_SIMILARITY.\n",
    "    \n",
    "# Let's check the structure for these arguments, using COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND\n",
    "# as example. All instructions are valid for COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND.\n",
    "    \n",
    "# COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND =\n",
    "# [{'left_df_column': None, 'right_df_column': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the column name on the left dataframe; and the second one contains the \n",
    "# correspondent column on the right dataframe.\n",
    "# The function will loop through all dictionaries in\n",
    "# this list, access the values of the key 'left_df_column' to retrieve a column to analyze in the left \n",
    "# dataframe; and access access the key 'righ_df_column' to obtain the correspondent column in the right\n",
    "# dataframe. Then, it will look for potential matches.\n",
    "# For COLUMNS_WHERE_EXACT_MATCHES_ARE_REQUIRED, only columns with perfect correspondence will be\n",
    "# retrieved. For COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND, when the algorithm finds a correspondence\n",
    "# that satisfies the threshold criterium, it will assign it as a match. \n",
    "# For instance, suppose you have a word written in too many ways, in a column named 'continent' that\n",
    "# should be used as key: \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" ...\n",
    "# Since they have sufficient similarity, they will be assigned as matching.\n",
    "    \n",
    "# The objects COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND and\n",
    "# COLUMNS_WHERE_EXACT_MATCHES_ARE_REQUIRED must be declared as lists, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'left_df_column' and 'right_df_column'.\n",
    "# Notice that this function performs fuzzy matching, so it MAY SEARCH substrings and strings\n",
    "# written with different cases (upper or lower) when this portions or modifications make the\n",
    "# strings sufficiently similar to each other.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'left_df_column': df_left_column, 'right_df_column': df_right_column}, \n",
    "# where df_left_column and df_right_column represent the strings for searching and replacement \n",
    "# (If the key contains None, the new dictionary will be ignored).\n",
    "\n",
    "    \n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = etl.RECORD_LINKAGE (df_left = DF_LEFT, df_right = DF_RIGHT, columns_to_block_as_basis_for_comparison = COLUMNS_TO_BLOCK_AS_BASIS_FOR_COMPARISON, columns_where_exact_matches_are_required = COLUMNS_WHERE_EXACT_MATCHES_ARE_REQUIRED, columns_where_similar_strings_should_be_found = COLUMNS_WHERE_SIMILAR_STRINGS_SHOULD_BE_FOUND, threshold_for_percent_of_similarity = THRESHOLD_FOR_PERCENT_OF_SIMILARITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "98ebf2e3-e978-4f95-9883-21c167373359"
   },
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1b8991d3-de61-48b6-8288-9b38f76b8520",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = etl.UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9cf49d56-7f8b-4a92-af68-693f4ea1b0ef"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "095e8246-4b24-4adf-b721-9b5c8f31af3b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = etl.df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e6c7348a-4992-475c-b575-3e0783e4065d"
   },
   "source": [
    "### **Dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d4728e3c-0303-4be1-9dd9-9ee1fbeb877c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "WHAT_TO_DROP = 'columns'\n",
    "# WHAT_TO_DROP = 'columns' for removing the columns specified by their names (headers)\n",
    "# in COLS_LIST (a list of strings).\n",
    "# WHAT_TO_DROP = 'rows' for removing the rows specified by their indices in\n",
    "# ROW_INDEX_LIST (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "# the first row is row number zero.\n",
    "\n",
    "COLS_LIST = None\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to be removed\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "# If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "# e.g. COLS_LIST = 'col1'; or COLS_LIST = ['col1']\n",
    "\n",
    "ROW_INDEX_LIST = None\n",
    "# ROW_INDEX_LIST = a list of integers containing the indices of the rows that will be dropped.\n",
    "# e.g. ROW_INDEX_LIST = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "# 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "# a list).\n",
    "# e.g. ROW_INDEX_LIST = 20 or ROW_INDEX_LIST = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = etl.drop_columns_or_rows (df = DATASET, what_to_drop = WHAT_TO_DROP, cols_list = COLS_LIST, row_index_list = ROW_INDEX_LIST, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0e18ad2c-4be6-4262-a467-6ee2d3956374"
   },
   "source": [
    "### **Removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8f478be4-0b27-46cd-9834-071d3c969b32",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_ANALYZE = None\n",
    "# if LIST_OF_COLUMNS_TO_ANALYZE = None, the whole dataset will be analyzed, i.e., rows\n",
    "# will be removed only if they have same values for all columns from the dataset.\n",
    "# Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "# same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "# being declared.\n",
    "# e.g. LIST_OF_COLUMNS_TO_ANALYZE = ['column1'] will check only 'column1'. Entries with same value\n",
    "# on 'column1' will be considered duplicates and will be removed.\n",
    "# LIST_OF_COLUMNS_TO_ANALYZE = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "# 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "# duplicates and will be removed.\n",
    "\n",
    "WHICH_ROW_TO_KEEP = 'first'\n",
    "# WHICH_ROW_TO_KEEP = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "# None or an invalid string is input, this method will be selected.\n",
    "# WHICH_ROW_TO_KEEP = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = etl.remove_duplicate_rows (df = DATASET, list_of_columns_to_analyze = LIST_OF_COLUMNS_TO_ANALYZE, which_row_to_keep = WHICH_ROW_TO_KEEP, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a5c57d9d-1bbd-4c1e-b30e-26058f13233a"
   },
   "source": [
    "### **Removing all columns and rows that contain only missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "46cbd866-2e2e-4aa4-9f12-ae202e0ce661",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_IGNORE = None\n",
    "# list_of_columns_to_ignore: if you do not want to check a specific column, pass its name\n",
    "# (header) as an element from this list. It should be declared as a list even if it contains\n",
    "# a single value.\n",
    "# e.g. list_of_columns_to_ignore = ['column1'] will not analyze missing values in column named\n",
    "# 'column1'; list_of_columns_to_ignore = ['col1', 'col2'] will ignore columns 'col1' and 'col2'\n",
    "\n",
    "# Cleaned dataframe returned as cleaned_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = etl.remove_completely_blank_rows_and_columns (df = DATASET, list_of_columns_to_ignore = LIST_OF_COLUMNS_TO_IGNORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b37ceb69-b06e-482b-955d-1c716c11461d"
   },
   "source": [
    "### **Characterizing the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b6fbeb9b-6ece-4c89-a228-29d65ef0fef2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = None\n",
    "# TIMESTAMP_TAG_COLUMN: name (header) of the column containing the timestamps. \n",
    "# Keep TIMESTAMP_TAG_COLUMN = None if the dataframe do not contain timestamps.\n",
    "\n",
    "# Dataframe with summary from the categorical variables returned as cat_vars_summary. \n",
    "# Simply modify this object on the left of equality:\n",
    "cat_vars_summary = etl.characterize_categorical_variables (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "37c83558-e5c5-43db-8a12-9466d4604f04"
   },
   "source": [
    "### **Grouping the dataframe by a timestamp**\n",
    "- Numeric variables aggregated in terms of a custom function, passed as `aggregation_function`;\n",
    "- Categorical variables aggregated in terms of mode, the most common value observed (maximum of the statistical distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "434885b9-ac4e-4317-8488-6c8b192526f2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be grouped\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "\n",
    "SUBSET_OF_COLUMNS_TO_AGGREGATE = None\n",
    "# SUBSET_OF_COLUMNS_TO_AGGREGATE: list of strings (inside quotes) containing the names \n",
    "# of the columns that will be aggregated. Use this argument if you want to aggregate only a subset,\n",
    "# not the whole dataframe. Declare as a list even if there is a single column to group by.\n",
    "# e.g. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"response_feature\"] will return the column \n",
    "# 'response_feature' grouped. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"col1\", 'col2'] will return columns\n",
    "# 'col1' and 'col2' grouped.\n",
    "# If you want to aggregate the whole subset, keep SUBSET_OF_COLUMNS_TO_AGGREGATE = None.\n",
    "\n",
    "GROUPING_FREQUENCY_UNIT = 'day'\n",
    "#Alternatively: 'year', 'month', 'week', 'hour', 'minute', 'day', or 'second'\n",
    "\n",
    "NUMBER_OF_PERIODS_TO_GROUP = 1 \n",
    "# Group by every NUMBER_OF_PERIODS_TO_GROUP = 1 periods (every day, if 'day' is selected).\n",
    "#Bin size. Alternatively: any integer number. Check the instructions in function comments.\n",
    "\n",
    "AGGREGATE_FUNCTION = 'mean'\n",
    "# Keep the method inside quotes.\n",
    "# Alternatively: use 'mean','sum', median','std', 'count', 'min','max','mode','geometric_mean',\n",
    "# 'harmonic_mean','kurtosis','skew','geometric_std','interquartile_range','mean_standard_error',\n",
    "# or 'entropy'\n",
    "\n",
    "# ADJUST OF GROUPING BASED ON A FIXED TIMESTAMP\n",
    "# You can specify the origin (start_time) or the offset (offset_time), which are equivalent.\n",
    "# WARNING: DECLARE ONLY ONE OF THESE PARAMETERS. DO NOT DECLARE AN OFFSET IF AN ORIGIN WAS \n",
    "# SPECIFIED, AND VICE-VERSA.\n",
    "START_TIME = None\n",
    "OFFSET_TIME = None\n",
    "# Alternatively, these parameters should be declared as a pandas Timestamp or in the\n",
    "# specific notation of Pandas offset_time for the Grouper class:\n",
    "# START_TIME = pd.Timestamp('2000-10-01 23:30:00', unit = 'ns')\n",
    "# Simply substitute the Timestamp inside quotes by the correct start timestamp.\n",
    "# This timestamp do not have to be complete, but must be interpretable by the Timestamp\n",
    "# function.\n",
    "# OFFSET_TIME = '23h30min', OFFSET_TIME = '2min', etc. Simply substitute the offset time\n",
    "# inside quotes by the correct value.\n",
    "# For examples on the notation for start and offset time, check Pandas grouper class\n",
    "# documentation, and Pandas timestamp class documentation:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COLUMN = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COLUMN = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COLUMN\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "\n",
    "\n",
    "# New dataframe saved as grouped_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "grouped_df = etl.GROUP_VARIABLES_BY_TIMESTAMP (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, subset_of_columns_to_aggregate = SUBSET_OF_COLUMNS_TO_AGGREGATE, grouping_frequency_unit = GROUPING_FREQUENCY_UNIT, number_of_periods_to_group = NUMBER_OF_PERIODS_TO_GROUP, aggregate_function = AGGREGATE_FUNCTION, start_time = START_TIME, offset_time = OFFSET_TIME, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COLUMN, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "33e0a2eb-7562-4d56-9874-0e7eb3d59105"
   },
   "source": [
    "### **Grouping the dataframe by a given variable**\n",
    "- Categorical variables are grouped by this function only when a proper aggregation function is selected, like the 'mode'.\n",
    "- If other aggregate is selected, only numeric variables are grouped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6bf67e35-368c-485b-bf2e-ba0c0e24c913"
   },
   "source": [
    "#### Case 1: return a statistics summary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "18f26891-9481-4536-84be-136b09813325",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "VARIABLE_TO_GROUP_BY = 'categorical_column_name'\n",
    "# of the column in terms of which the dataframe will be grouped by. e.g. \n",
    "# VARIABLE_TO_GROUP_BY = \"column1\" will group the dataframe in terms of 'column1'.\n",
    "# WARNING: do not use this function to group a dataframe in terms of a timestamp. To group by\n",
    "# a timestamp, use function GROUP_VARIABLES_BY_TIMESTAMP instead.\n",
    "\n",
    "RETURN_SUMMARY_DATAFRAME = True\n",
    "# RETURN_SUMMARY_DATAFRAME = False. Set RETURN_SUMMARY_DATAFRAME = True if you want the function\n",
    "# to return a dataframe containing summary statistics (obtained with the describe method).\n",
    "\n",
    "SUBSET_OF_COLUMNS_TO_AGGREGATE = None\n",
    "# SUBSET_OF_COLUMNS_TO_AGGREGATE: list of strings (inside quotes) containing the names \n",
    "# of the columns that will be aggregated. Use this argument if you want to aggregate only a subset,\n",
    "# not the whole dataframe. Declare as a list even if there is a single column to group by.\n",
    "# e.g. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"response_feature\"] will return the column \n",
    "# 'response_feature' grouped. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"col1\", 'col2'] will return columns\n",
    "# 'col1' and 'col2' grouped.\n",
    "# If you want to aggregate the whole subset, keep SUBSET_OF_COLUMNS_TO_AGGREGATE = None.\n",
    "\n",
    "AGGREGATE_FUNCTION = 'mean'\n",
    "# AGGREGATE_FUNCTION = 'mean': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "# 'standard_deviation', 'cum_sum', 'cum_prod', 'cum_max', 'cum_min',\n",
    "# '10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range',\n",
    "# 'mean_standard_error', 'entropy'\n",
    "# To use another aggregate function, you can use the .agg method, passing \n",
    "# the aggregate as argument, such as in:\n",
    "# .agg(scipy.stats.mode), \n",
    "# where the argument is a Scipy aggregate function.\n",
    "# If None or an invalid function is input, 'mean' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COLUMN = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COLUMN = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COLUMN\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "\n",
    "\n",
    "# Grouped dataframe, and summary statistics dataframe returned as:\n",
    "# grouped_df and summary_agg_df, respectively.\n",
    "# Simply modify these objects on the left of equality:\n",
    "grouped_df, summary_agg_df = etl.GROUP_DATAFRAME_BY_VARIABLE (df = DATASET, variable_to_group_by = VARIABLE_TO_GROUP_BY, return_summary_dataframe = RETURN_SUMMARY_DATAFRAME, subset_of_columns_to_aggregate = SUBSET_OF_COLUMNS_TO_AGGREGATE, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COLUMN, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "37abe72e-4dff-4510-a0f6-2c13d27a9101"
   },
   "source": [
    "#### Case 2: do not return a statistics summary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "db37dd5b-d017-4433-af46-b32e8c37bbb7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "VARIABLE_TO_GROUP_BY = 'categorical_column_name'\n",
    "# of the column in terms of which the dataframe will be grouped by. e.g. \n",
    "# VARIABLE_TO_GROUP_BY = \"column1\" will group the dataframe in terms of 'column1'.\n",
    "# WARNING: do not use this function to group a dataframe in terms of a timestamp. To group by\n",
    "# a timestamp, use function GROUP_VARIABLES_BY_TIMESTAMP instead.\n",
    "\n",
    "RETURN_SUMMARY_DATAFRAME = False\n",
    "# RETURN_SUMMARY_DATAFRAME = False. Set RETURN_SUMMARY_DATAFRAME = True if you want the function\n",
    "# to return a dataframe containing summary statistics (obtained with the describe method).\n",
    "\n",
    "SUBSET_OF_COLUMNS_TO_AGGREGATE = None\n",
    "# SUBSET_OF_COLUMNS_TO_AGGREGATE: list of strings (inside quotes) containing the names \n",
    "# of the columns that will be aggregated. Use this argument if you want to aggregate only a subset,\n",
    "# not the whole dataframe. Declare as a list even if there is a single column to group by.\n",
    "# e.g. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"response_feature\"] will return the column \n",
    "# 'response_feature' grouped. SUBSET_OF_COLUMNS_TO_AGGREGATE = [\"col1\", 'col2'] will return columns\n",
    "# 'col1' and 'col2' grouped.\n",
    "# If you want to aggregate the whole subset, keep SUBSET_OF_COLUMNS_TO_AGGREGATE = None.\n",
    "\n",
    "AGGREGATE_FUNCTION = 'mean'\n",
    "# AGGREGATE_FUNCTION = 'mean': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "# 'standard_deviation', 'cum_sum', 'cum_prod', 'cum_max', 'cum_min',\n",
    "# '10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range',\n",
    "# 'mean_standard_error', 'entropy'\n",
    "# To use another aggregate function, you can use the .agg method, passing \n",
    "# the aggregate as argument, such as in:\n",
    "# .agg(scipy.stats.mode), \n",
    "# where the argument is a Scipy aggregate function.\n",
    "# If None or an invalid function is input, 'mean' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COLUMN = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COLUMN = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COLUMN\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "\n",
    "\n",
    "# Grouped dataframe returned as grouped_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "grouped_df = etl.GROUP_DATAFRAME_BY_VARIABLE (df = DATASET, variable_to_group_by = VARIABLE_TO_GROUP_BY, return_summary_dataframe = RETURN_SUMMARY_DATAFRAME, subset_of_columns_to_aggregate = SUBSET_OF_COLUMNS_TO_AGGREGATE, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COLUMN, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "283c2d28-ca92-4625-ae10-1c9f30b03378"
   },
   "source": [
    "### **Creating columns with isolated information from the timestamps**\n",
    "- Columns containing isolated information from the timestamp (each column with a given information): \n",
    "    - Values of year, month, week, day, hour, minute, or second may be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e75b493e-ac2d-447e-9860-4861cc451927",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "#Keep inside quotes.\n",
    "\n",
    "LIST_OF_INFO_TO_EXTRACT = ['year', 'month', 'week', 'day']\n",
    "# LIST_OF_INFO_TO_EXTRACT: list of information to extract from the timestamp. Each information\n",
    "# will be extracted as a separate column. The allowed values are:\n",
    "# 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'. Declare as a list even if only\n",
    "# one information is going to be extracted. For instance:\n",
    "# LIST_OF_INFO_TO_EXTRACT = ['second'] extracts only the second.\n",
    "# LIST_OF_INFO_TO_EXTRACT = ['year', 'month', 'week', 'day'] extracts year, month, week and day. \n",
    "\n",
    "LIST_OF_NEW_COLUMN_NAMES = None\n",
    "# list_of_new_column_names: list of names (strings) of the new created columns. \n",
    "# If no value is provided, it will be equals to extracted_info. For instance: if\n",
    "# list_of_info_to_extract = ['year', 'month', 'week', 'day'] and list_of_new_column_names = None,\n",
    "# the new columns will be named as 'year', 'month', 'week', and 'day'.\n",
    "# WARNING: This list must contain the same number of elements of list_of_info_to_extract and both\n",
    "# must be in the same order. Considering the same example of list, if list_of_new_column_names =\n",
    "# ['col1', 'col2', 'col3', 'col4'], 'col1' will be referrent to 'year', 'col2' to 'month', 'col3'\n",
    "# to 'week', and 'col4' to 'day'\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.EXTRACT_TIMESTAMP_INFO (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, list_of_info_to_extract = LIST_OF_INFO_TO_EXTRACT, list_of_new_column_names = LIST_OF_NEW_COLUMN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "41b39b6c-aa4c-4c55-a1a0-17b2a21beebd"
   },
   "source": [
    "### **Calculating differences between successive timestamps (delays)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ede4e61c-3ad3-489b-b799-34040262d735"
   },
   "source": [
    "#### Case 1: return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2c54dc14-657c-4590-b29c-021994917c4b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. NEW_TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = 'day'\n",
    "# Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = True\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality.\n",
    "# Average delay float value istored into variable avg_delay. \n",
    "# Simply modify this object on the left of equality.\n",
    "new_df, avg_delay = etl.CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b8f6060b-55c9-4a4b-9ec3-3456e2f55d02"
   },
   "source": [
    "#### Case 2: do not return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f013415c-93c3-43c1-b68d-fbc868af2551",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = None\n",
    "# Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = False\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1154e922-ce20-4caa-9238-2215c89cd9ef"
   },
   "source": [
    "### **Calculating differences between timestamps (timedeltas)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "67ac4f38-f652-4ab7-ac3f-1e8dde60a6a2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN1 = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN2 = \"TIMESTAMP2\"\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the right, that will be substracted from the timestamp on the left.\n",
    "# Keep inside quotes.\n",
    "# e.g. TIMESTAMP_TAG_COLUMN2 = \"timestamp_grouped_delayed\" will subtract the delayed version of\n",
    "# \"timestamp_grouped\" created by the function CALCULATE_DELAY\n",
    "\n",
    "TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = 'day'\n",
    "# Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.CALCULATE_TIMEDELTA (df = DATASET, timestamp_tag_column1 = TIMESTAMP_TAG_COLUMN1, timestamp_tag_column2 = TIMESTAMP_TAG_COLUMN2, timedelta_column_name  = TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5540857b-8d9e-4d59-80fe-dd614942cf6a"
   },
   "source": [
    "### **Adding or subtracting a timedelta from a timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b58173fa-47a9-4b83-89a4-213e36a7e759",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "TIMEDELTA = 2\n",
    "# Numeric value of the timedelta.\n",
    "# WARNING: simply input a numeric value, not a string with unit. e.g. timedelta = 2.4\n",
    "# If you want to subtract a timedelta, input a negative value. e.g. timedelta = - 2.4\n",
    "# Alternatively, input any desired real number.\n",
    "\n",
    "NEW_TIMESTAMP_COL = None\n",
    "# Name of the new column containing the obtained timestamp.  If no value is provided, the \n",
    "# default name [timestamp_tag_column]+[timedelta] will be given.\n",
    "# Alternatively, input a string value inside quotes with the name of this new column.\n",
    "# e.g. NEW_TIMESTAMP_COL = \"new_timestamp\"\n",
    "\n",
    "TIMEDELTA_UNIT = 'day'\n",
    "# Unit of the timedelta interval. If no value is provided, the unit will be considered 'ns' \n",
    "# (default). \n",
    "# Possible values are: TIMEDELTA_UNIT = None, 'day', 'hour', 'minute', 'second', or 'ns'.\n",
    "# Keep the unit inside quotes. \n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.ADD_TIMEDELTA (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, timedelta = TIMEDELTA, new_timestamp_col  = NEW_TIMESTAMP_COL, timedelta_unit = TIMEDELTA_UNIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "af49e77c-df13-424e-bf6d-ff288b0a3118"
   },
   "source": [
    "### **Slicing the dataframe (selecting a specific subset of rows)**\n",
    "- This function maintains all columns from the original dataframe, returning a dataframe that is a subset of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3531a73b-c263-4938-9086-7272a7006ac3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "FROM_ROW = 'first_only'\n",
    "TO_ROW = 'only'\n",
    "# FROM_ROW and to_row: integer or strings:\n",
    "# FROM_ROW may be any integer from 0 to the last row of the dataset\n",
    "# and the following strings: 'first' and 'first_only'\n",
    "# TO_ROW may be any integer from 0 to the last row of the dataset\n",
    "# and the following strings: 'last', 'last_only', and 'only'\n",
    "    \n",
    "# the combination FROM_ROW = 'first', TO_ROW = 'last' will\n",
    "# return the original dataframe itself.\n",
    "# The same is valid for the combination FROM_ROW = 'first_only', \n",
    "# TO_ROW = 'last_only'; or of combinations between FROM_ROW = 0\n",
    "# (index of the first row) with 'last' or the index\n",
    "# of the last row; or combinations between 'first' and the index\n",
    "# of the last row.\n",
    "    \n",
    "# These possibilities are the first checked by the code. If none\n",
    "# of these special cases are present, then:\n",
    "    \n",
    "# FROM_ROW = 'first_only' selects a dataframe containing only the\n",
    "# first row, independently of the parameter passed as TO_ROW;\n",
    "\n",
    "# TO_ROW = 'last_only' selects a dataframe containing only the\n",
    "# last row, independently of the parameter passed as FROM_ROW;\n",
    "    \n",
    "# if TO_ROW = 'only', the sliced dataframe will be formed by only the\n",
    "# row passed as FROM_ROW (an integer representing the row index is\n",
    "# passed) - explained in the following lines\n",
    "    \n",
    "# These three special cases are dominant over the following ones\n",
    "# (they are checked firstly, and force the modifying of slicing limits):\n",
    "# Other special cases:\n",
    "    \n",
    "# FROM_ROW = 'first' starts slicing on the first row (index 0) -\n",
    "# the 1st row from the dataframe will be the 1st row of the sliced\n",
    "# dataframe too.\n",
    "    \n",
    "# TO_ROW = 'last' finishes slicing in the last row - the last row\n",
    "# from the dataframe will be the last row of the sliced dataframe.\n",
    "    \n",
    "# If i and j are integer numbers, they represent the indices of rows:\n",
    "# FROM_ROW = i starts the sliced dataframe from the row of index i\n",
    "# of the original dataframe.\n",
    "# e.g. FROM_ROW = 8 starts the slicing from row with index 8. Since\n",
    "# slicing starts from 0, this is the 9th row of the original dataframe.\n",
    "# TO_ROW = j finishes the sliced dataframe on the row of index j of\n",
    "# the original dataframe. Attention: this row with index j is included,\n",
    "# and will be the last_row of the sliced dataframe.\n",
    "# e.g. if TO_ROW = 21, the last row of the sliced dataframe will be the\n",
    "# row with index 21 of the original dataframe. Since slicing starts\n",
    "# from 0, this is the 22nd row of the original dataframe.\n",
    "    \n",
    "# In summary, if FROM_ROW = 8, TO_ROW = 21, the sliced dataframe\n",
    "# will be formed from the row of index 8 to the row of index 21 of\n",
    "# the original dataframe, including both the row of index 8 and the row\n",
    "# index 21. \n",
    "# FROM_ROW is effectively the first row of the new dataframe;\n",
    "# and TO_ROW is effectively the last row of the new dataframe.\n",
    "# Notice that the use of TO_ROW < FROM_ROW will raise an error.\n",
    "\n",
    "RESTART_INDEX_OF_THE_SLICED_DATAFRAME = False\n",
    "# RESTART_INDEX_OF_THE_SLICED_DATAFRAME = False to keep the \n",
    "# same row index of the original dataframe; or \n",
    "# RESTART_INDEX_OF_THE_SLICED_DATAFRAME = True to reset indices \n",
    "# (start a new index, from 0 for the first row of the \n",
    "# returned dataframe).\n",
    "    \n",
    "# New dataframe saved as sliced_df. Simply modify this object on the left of equality:\n",
    "sliced_df = etl.SLICE_DATAFRAME (df = DATASET, from_row = FROM_ROW, to_row = TO_ROW, restart_index_of_the_sliced_dataframe = RESTART_INDEX_OF_THE_SLICED_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9dcde567-a5a2-499e-963c-075cdeca860f"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "07f821b6-a1dc-4bd2-88ea-80086356da01",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "idsw.export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f1178b00-a1e5-439b-9089-400dcff0faa0"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "51cd375d-7db3-4886-90b7-2887969ec917"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3feead95-006c-4267-9002-29f245079755",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b913de9a-e166-4807-a45a-7bd8e2fbba3d"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7981965d-93a9-40e9-884b-42f3b22e2718",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6fe453d2-8e55-4a53-b8b0-f094928b3ddd"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "219e8dcf-0b8d-4c1e-bd69-79af37977def",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
