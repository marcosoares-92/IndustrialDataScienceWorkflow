{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1ab4844b-d67f-4d6a-8f46-1cbbafedd295"
   },
   "source": [
    "# **Seggregation of the Dataset and Mean Difference Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "38991445-711f-4f1c-ab86-efd591c77748"
   },
   "source": [
    "## _ETL Workflow Notebook 5_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "13e2dfdb-ad4d-4aa3-b6d3-f0129e930dd2"
   },
   "source": [
    "## Content:\n",
    "1. Calculating general statistics for a given column; \n",
    "2. Getting data quantiles for a given column; \n",
    "3. Getting a particular P-percent quantile limit for a given column;  \n",
    "4. Selecting subsets from a dataframe (using row filters) and labelling these subsets; \n",
    "5. Performing Analysis of Variance (ANOVA); and obtaining box plots or violin plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e8ed4d7b-6f0e-4a33-8026-388d7d4bb96e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.oneway import anova_oneway\n",
    "import idsw\n",
    "from idsw import etl\n",
    "from idsw.etl import etl_workflow as ewf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e521fe7f-a726-451c-a7c9-7d5106a0b77a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cfe0d9a8-77ff-41ec-bae4-28330fa0abdc",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ce32016f-787f-4698-94bc-6ce8f266bbd2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cb38656c-6f38-4c25-84c6-b39825769b48",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "41948917-24b6-4c93-bd39-6eac9e01325b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ccf6222b-679e-4a5e-a701-0659e1f1425a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "15141de4-8b53-4a0f-adcb-6fcec11cf703",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1d9b97e1-3619-4bed-ad7a-35e0b2282e78"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "67e0a3ac-1613-4134-8e3c-2f0caa9581a8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = ewf.select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "41f2019b-8ace-48aa-8f8a-11943f4deb9d"
   },
   "source": [
    "### **Applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5c52932a-8ad0-43d8-832a-2af2354f7d5c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = ewf.APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1b84683e-2a26-4c84-88b3-1df8565a928b"
   },
   "source": [
    "### **Calculating general statistics for a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "96e65550-8710-49e6-9c4a-f6c9e0c4cb7f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Statistics dataframe saved as general_stats. \n",
    "# Simply modify this object on the left of equality:\n",
    "general_stats = ewf.COLUMN_GENERAL_STATISTICS (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "62e1d1a0-f361-455a-b9b1-ee40f944675b"
   },
   "source": [
    "### **Getting data quantiles for a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cccab750-094d-41d7-a5e6-207f2c3d3cd3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Quantiles dataframe saved as quantiles_summ_df\n",
    "# Simply modify this object on the left of equality:\n",
    "quantiles_summ_df = ewf.GET_QUANTILES_FOR_COLUMN (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ef53bc52-4c17-45b6-96b2-c99e031089b6"
   },
   "source": [
    "### **Getting a particular P-percent quantile limit (P is a percent, from 0 to 100) for a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "fdfd63d9-e32e-4eaa-b9d8-0d4a4cf5649e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "P_PERCENT = 100\n",
    "# P_PERCENT: float value from 0 to 100 representing the percent of the quantile\n",
    "# if P_PERCENT = 31.2, then 31.2% of the data will fall below the returned value\n",
    "# if P_PERCENT = 75, then 75% of the data will fall below the returned value\n",
    "# if P_PERCENT = 0, the minimum value is returned.\n",
    "# if P_PERCENT = 100, the maximum value is returned.\n",
    "\n",
    "# P-Percent quantile limit returned as the float value quantile_lim\n",
    "# Simply modify this variable on the left of equality:\n",
    "quantile_lim = ewf.GET_P_PERCENT_QUANTILE_LIM_FOR_COLUMN (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, p_percent = P_PERCENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1e1d6f04-4d70-4c28-a1df-91f454330644"
   },
   "source": [
    "### **Selecting subsets from a dataframe (using row filters) and labelling these subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5590295c-ca41-4058-bcb2-06146532c4be",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Attention: this function selects subsets from the dataframe and label them, \n",
    "# allowing the seggregation of the data.\n",
    "# If you want to filter the dataframe to eliminate non-selected rows, use the \n",
    "# function APPLY_ROW_FILTERS_LIST\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_LABELS = [\n",
    "    \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}, \n",
    "    {'filter': None, \n",
    "     'value_to_apply': None, 'new_column_name': None}\n",
    "    \n",
    "]\n",
    "\n",
    "# LIST_OF_LABELS = [{'filter': None, 'value_to_apply': None, 'new_column_name': None}]\n",
    "# LIST_OF_LABELS is as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'filter' for one of the boolean filters that will be applied; \n",
    "# 'value_to_apply' the value that will be used for labelling that specific subset selected\n",
    "# from the boolean filter (it may be either a string or a value); and\n",
    "# 'new_column_name': a string or variable to be the name of the new column created. If None,\n",
    "# a standard name will be applied.\n",
    "\n",
    "## ATTENTION: If you want the labels to be applied to a same column, declare the exact same value\n",
    "# for the key 'new_column_name'. Also, if you want the value to be applied to an existing column,\n",
    "# declare the existing column's name in 'new_column_name'.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'filter': filter, 'value_to_apply': value, 'new_column_name': name}, where \n",
    "# filter, value, and name represent the boolean filter, the value for labelling, and the new\n",
    "# column name (you can pass 'value_to_apply': None, 'new_column_name': None, but if \n",
    "# 'filter' is None, the new dictionary will be ignored).\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "# boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "# boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "# Labelled dataframe saved as labelled_df\n",
    "# Simply modify this object on the left of equality:\n",
    "labelled_df = ewf.LABEL_DATAFRAME_SUBSETS (df = DATASET, list_of_labels = LIST_OF_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ec08f6a5-49bf-4d9f-8033-e2497240f01e"
   },
   "source": [
    "### **Performing Analysis of Variance (ANOVA); and for obtaining box plots or violin plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ddc2a3db-a19b-4ee7-abe6-983656336160",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "PLOT_TYPE = 'box'\n",
    "# PLOT_TYPE = 'box' to plot a boxplot.\n",
    "# PLOT_TYPE = 'violin' to plot a violinplot.\n",
    "# If PLOT_TYPE = None, or PLOT_TYPE = 'only_anova', only the anova analysis will be performed.\n",
    "\n",
    "CONFIDENCE_LEVEL_PERCENT = 95\n",
    "# CONFIDENCE_LEVEL_PERCENT = 95 = 95% confidence\n",
    "# It is the percent of confidence for the analysis.\n",
    "# Set CONFIDENCE_LEVEL_PERCENT = 90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce confidence_level_pct\n",
    "# to get less restrictive results.\n",
    "\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' for vertical plots; \n",
    "# ORIENTATION = 'horizontal', for horizontal plots.\n",
    "    \n",
    "REFERENCE_VALUE = None\n",
    "# REFERENCE_VALUE: keep it as None or add a float value.\n",
    "# This reference value will be shown as a red constant line to be compared\n",
    "# with the plots. e.g. REFERENCE_VALUE = 1.0 will plot a red line passing through\n",
    "# VARIABLE_TO_ANALYZE = 1.0\n",
    "\n",
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_LABELS_OR_GROUPS = None # Alternatively: correct name for X-column\n",
    "VARIABLE_TO_ANALYZE = None # Alternatively: correct name for Y-column\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the label or group indication as COLUMN_WITH_LABELS_OR_GROUPS; and the column \n",
    "# containing the variable to analyze as VARIABLE_TO_ANALYZE.\n",
    "\n",
    "# If COLUMN_WITH_LABELS_OR_GROUPS is None, the ANOVA analysis will not be performed and \n",
    "# the plot will be obtained for the whole series.\n",
    "\n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes.     \n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results'. If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_LABELS_OR_GROUPS = 'group',\n",
    "# VARIABLE_TO_ANALYZE = 'results'.\n",
    "\n",
    "\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_LABELS_OR_GROUPS = None, VARIABLE_TO_ANALYZE = None).\n",
    "    \n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    {'values_to_analyze': None, 'label': None}, \n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: {'values_to_analyze': None, 'label': None}\n",
    "# if data is already converted to series, lists or arrays, provide them as a list of dictionaries. \n",
    "# It must be declared as a list, in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'values_to_analyze' for values that will be analyzed, and 'label' for\n",
    "# the label or group correspondent to the series (may be a number or a string). \n",
    "# If you do not want to declare a series, simply keep as None, but do not remove or rename a \n",
    "# key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'values_to_analyze': y, 'label': 'series_y'}, where y represents the values\n",
    "# to analyze, and 'series_y' is the label \n",
    "# (you can pass 'label': None, but if values_to_analyze' is None, the new \n",
    "# dictionary will be ignored).\n",
    "    \n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [{'values_to_analyze': y, 'label': 0}]\n",
    "# will plot a single variable. In turns: LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'values_to_analyze': DATASET['Y1'], 'label': 'label1'}, \n",
    "# {'values_to_analyze': DATASET['Y2'], 'label': 'label2'}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 and Y2.\n",
    "# Notice that all dictionaries where 'values_to_analyze' is None are automatically ignored.\n",
    "# If None is provided to 'label', an automatic label will be generated.\n",
    "\n",
    "## Parameters with effect only for boxplots (PLOT_TYPE = 'box'):\n",
    "OBTAIN_BOXPLOT_WITH_FILLED_BOXES = True\n",
    "# Manipulate parameter patch_artist (boolean, default: False)    \n",
    "# If OBTAIN_BOXPLOT_WITH_FILLED_BOXES = True, the boxes are created filled.     \n",
    "# If OBTAIN_BOXPLOT_WITH_FILLED_BOXES = False, only the contour of the boxes are shown    \n",
    "# (obtain void white boxes).\n",
    "OBTAIN_BOXPLOT_WITH_NOTCHED_BOXES = False\n",
    "# OBTAIN_BOXPLOT_WITH_NOTCHED_BOXES = False    \n",
    "# Manipulate parameter notch (boolean, default: False) from the boxplot object    \n",
    "# Whether to draw a notched boxplot (OBTAIN_BOXPLOT_WITH_NOTCHED_BOXES = True), \n",
    "# or a rectangular boxplot (OBTAIN_BOXPLOT_WITH_NOTCHED_BOXES = False).     \n",
    "# The notches represent the confidence interval (CI) around the median.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# '{PLOT_TYPE}_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# Dictionary storing ANOVA F-test and p-value returned as anova_summary_dict\n",
    "# Dictionary mapping each component of the box or violin plot returned as plot_returned_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "anova_summary_dict, plot_returned_dict = ewf.anova_box_violin_plot (plot_type = PLOT_TYPE, confidence_level_pct = CONFIDENCE_LEVEL_PERCENT, orientation = ORIENTATION, reference_value = REFERENCE_VALUE, data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_labels_or_groups = COLUMN_WITH_LABELS_OR_GROUPS, variable_to_analyze = VARIABLE_TO_ANALYZE, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, obtain_boxplot_with_filled_boxes = OBTAIN_BOXPLOT_WITH_FILLED_BOXES, obtain_boxplot_with_notched_boxes = OBTAIN_BOXPLOT_WITH_NOTCHED_BOXES, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0e89c203-9f96-4b1b-bdad-f3cd485650a8"
   },
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "063c787f-7d9d-4304-bad3-8cba3d9fd7ac",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = ewf.UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8cd150b1-0234-4f02-9faf-5c3b0fbeaa4d",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6103e457-1576-4fba-b834-7e6d7421b090"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e0eeefaf-1f79-4d57-9e99-a7f0fd87e27b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1c9bb1ab-f2a7-453c-9bf6-f4ea8da12ef5"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "450aa7be-dd23-4f9f-8972-46d50aba05e0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7fa51838-60b9-4e08-a492-a434aaa976cc"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a907274a-5234-4440-955a-49cb1746329f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a6de03bf-0734-48a6-a563-6362864e4598"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "84b178bc-debd-4871-a445-d2d6e102d79d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5a0750e5-a23a-4e6d-9b01-63c1ebc88080"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9d355418-6646-41cd-bb30-ea805bd8f2d9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = ewf.df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f5b9d84d-9067-4fbc-a025-8b086243a1d8"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c5225cda-c5c9-4382-84ed-ebac1e0e3b66",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "idsw.export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5e8378cb-d6e9-4f66-bc18-e9ce5a135fea"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ae0714ee-fd44-42c3-be74-0ffa1d0f5676"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7437f830-3ea4-430a-94d4-4f31a13564a1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "118a671b-3eba-4485-b786-e8bd5e092ef9"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "746d5795-47d5-4b7f-9338-29572dd237c6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "48a931ef-9a96-4f12-9759-5eff1010afac"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a09417c9-d4ea-4d46-a6a8-8905d5107767",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
