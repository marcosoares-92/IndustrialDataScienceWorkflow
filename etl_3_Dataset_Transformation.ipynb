{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8e0cea1f-1e08-418e-b2e0-53178e19d71a"
   },
   "source": [
    "# **Dataset Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6689b182-e386-4ad3-a19e-215c07e2f6c1"
   },
   "source": [
    "## _ETL Workflow Notebook 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f04f7076-17fb-41c1-94e9-348059de0fce"
   },
   "source": [
    "## Content:\n",
    "1. Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type;\n",
    "2. Capitalizing or lowering case of string variables (string homogenizing);\n",
    "3. Substituting (replacing) substrings on string variables;\n",
    "4. Substituting (replacing or switching) whole strings by different text values (on string variables);\n",
    "5. Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings;\n",
    "6. Transforming the dataset and reverse transforms: log-transform; \n",
    "7. Exponential transform; \n",
    "8. Box-Cox transform; \n",
    "9. One-Hot Encoding;\n",
    "10. Ordinal Encoding;\n",
    "11. Feature scaling; \n",
    "12. Importing or exporting models and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "fec5846c-2b1d-4a60-b6fc-554077ea8582",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import idsw\n",
    "from idsw import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "81613f50-c3d7-4840-b8c4-e82fc7bb9e53",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7dcbbeac-5d1b-483a-83c1-a0a85c4a908f",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cf7dd573-3da1-4bec-95c0-46fc3d27b7ff",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9dbf7189-4ba9-4ffa-88a2-965699df42b9",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "29759209-4342-4e2c-addd-34acf841b18c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3afd2423-139d-431b-9a56-8e75400b2491",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e8d2a160-c74b-4d65-b7c2-e50e6f7f32c0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "METHOD = 'trim'\n",
    "# METHOD = 'trim' will eliminate trailing and leading white spaces from the strings in\n",
    "# COLUMN_TO_ANALYZE.\n",
    "# METHOD = 'substring' will eliminate a defined trailing and leading substring from\n",
    "# COLUMN_TO_ANALYZE.\n",
    "\n",
    "SUBSTRING_TO_ELIMINATE = None\n",
    "# SUBSTRING_TO_ELIMINATE = None. Set as a string (in quotes) if METHOD = 'substring'.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains time information: each string ends in \" min\":\n",
    "# \"1 min\", \"2 min\", \"3 min\", etc. If SUBSTRING_TO_ELIMINATE = \" min\", this portion will be\n",
    "# eliminated, resulting in: \"1\", \"2\", \"3\", etc. If NEW_VARIABLE_TYPE = None, these values will\n",
    "# continue to be strings. By setting NEW_VARIABLE_TYPE = 'int' or 'float', the series will be\n",
    "# converted to a numeric type.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_trim'\n",
    "# NEW_COLUMN_SUFFIX = \"_trim\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_trim\", the new column will be named as\n",
    "# \"column1_trim\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = etl.trim_spaces_or_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, new_variable_type = NEW_VARIABLE_TYPE, method = METHOD, substring_to_eliminate = SUBSTRING_TO_ELIMINATE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "94411320-e6a4-49b2-aa70-ee39a3630a6b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Capitalizing or lowering case of string variables (string homogenizing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "47e1604b-6bbf-43fe-9e4b-a4e60f2802a5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "METHOD = 'lowercase'\n",
    "# METHOD = 'capitalize' will capitalize all letters from the input string \n",
    "# (turn them to upper case).\n",
    "# METHOD = 'lowercase' will make the opposite: turn all letters to lower case.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains strings such as 'String One', 'STRING 2',  and\n",
    "# 'string3'. If METHOD = 'capitalize', the output will contain the strings: \n",
    "# 'STRING ONE', 'STRING 2', 'STRING3'. If METHOD = 'lowercase', the outputs will be:\n",
    "# 'string one', 'string 2', 'string3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_homogenized'\n",
    "# NEW_COLUMN_SUFFIX = \"_homogenized\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_homogenized\", the new column will be named as\n",
    "# \"column1_homogenized\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = etl.capitalize_or_lower_string_case (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, method = METHOD, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing) substrings on string variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = None\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = ''\n",
    "# SUBSTRING_TO_BE_REPLACED = None; new_substring_for_replacement = ''. \n",
    "# Strings (in quotes): when the sequence of characters SUBSTRING_TO_BE_REPLACED was\n",
    "# found in the strings from column_to_analyze, it will be substituted by the substring\n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT. If None is provided to one of these substring arguments,\n",
    "# it will be substituted by the empty string: ''\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains the following strings, with a spelling error:\n",
    "# \"my collumn 1\", 'his collumn 2', 'her column 3'. We may correct this error by setting:\n",
    "# SUBSTRING_TO_BE_REPLACED = 'collumn' and NEW_SUBSTRING_FOR_REPLACEMENT = 'column'. The\n",
    "# function will search for the wrong group of characters and, if it finds it, will substitute\n",
    "# by the correct sequence: \"my column 1\", 'his column 2', 'her column 3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_substringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_substringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_substringReplaced\", the new column will be named as\n",
    "# \"column1_substringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = etl.replace_substring (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a4c5fc1f-f93d-4f21-8fe3-d74c7658cb9b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing or switching) whole strings by different text values (on string variables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f91631b5-37db-4bd2-a72d-256859c9bd67",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = [\n",
    "    \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': None, 'new_string': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original string; and the second one contains the new string\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'original_string', and search these values on the strings\n",
    "# in COLUMN_TO_ANALYZE. When the value is found, it will be replaced (switched) by the correspondent\n",
    "# value in key 'new_string'.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'original_string' for the original strings to search on the column \n",
    "# column_to_analyze; and 'new_string', for the strings that will replace the original ones.\n",
    "# Notice that this function will not search for substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_to_analyze' and 'original_string'.\n",
    "# So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'original_string': original_str, 'new_string': new_str}, \n",
    "# where original_str and new_str represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'sunday', 'monday', 'tuesday', 'wednesday',\n",
    "# 'thursday', 'friday', 'saturday', but you want to obtain data labelled as 'weekend' or 'weekday'.\n",
    "# Set: LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': 'sunday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'saturday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'monday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'tuesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'wednesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'thursday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'friday', 'new_string': 'weekday'}]\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = etl.switch_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, list_of_dictionaries_with_original_strings_and_replacements = LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "dd1801c2-ceef-441f-883a-9900a9f5dbb0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1f7e4731-aaba-4ea4-8760-665522f2bb08",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "MODE = 'find_and_replace'\n",
    "# MODE = 'find_and_replace' will find similar strings; and switch them by one of the\n",
    "# standard strings if the similarity between them is higher than or equals to the threshold.\n",
    "# Alternatively: MODE = 'find' will only find the similar strings by calculating the similarity.\n",
    "\n",
    "THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0\n",
    "# THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0 - 0.0% means no similarity and 100% means equal strings.\n",
    "# The THRESHOLD_FOR_PERCENT_OF_SIMILARITY is the minimum similarity calculated from the\n",
    "# Levenshtein (minimum edit) distance algorithm. This distance represents the minimum number of\n",
    "# insertion, substitution or deletion of characters operations that are needed for making two\n",
    "# strings equal.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT = [\n",
    "    \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}\n",
    "    \n",
    "]\n",
    "# This is a list of dictionaries, where each dictionary contains a single key-value pair:\n",
    "# the key must be always 'standard_string', and the value will be one of the standard strings \n",
    "# for replacement: if a given string on the COLUMN_TO_ANALYZE presents a similarity with one \n",
    "# of the standard string equals or higher than the THRESHOLD_FOR_PERCENT_OF_SIMILARITY, it will be\n",
    "# substituted by this standard string.\n",
    "# For instance, suppose you have a word written in too many ways, making it difficult to use\n",
    "# the function switch_strings: \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" ...\n",
    "# You can use this function to search strings similar to \"Europe\" and replace them.\n",
    "    \n",
    "# The function will loop through all dictionaries in this list, access the values of the keys \n",
    "# 'standard_string', and search these values on the strings in COLUMN_TO_ANALYZE. When the value \n",
    "# is found, it will be replaced (switched) if the similarity is sufficiently high.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'standard_string'.\n",
    "# Notice that this function performs fuzzy matching, so it MAY SEARCH substrings and strings\n",
    "# written with different cases (upper or lower) when this portions or modifications make the\n",
    "# strings sufficiently similar to each other.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same key: {'standard_string': other_std_str}, \n",
    "# where other_std_str represents the string for searching and replacement \n",
    "# (If the key contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'California', 'Cali', 'Calefornia', \n",
    "# 'Calefornie', 'Californie', 'Calfornia', 'Calefernia', 'New York', 'New York City', \n",
    "# but you want to obtain data labelled as the state 'California' or 'New York'.\n",
    "# Set: list_of_dictionaries_with_standard_strings_for_replacement = \n",
    "# [{'standard_string': 'California'},\n",
    "# {'standard_string': 'New York'}]\n",
    "    \n",
    "# ATTENTION: It is advisable for previously searching the similarity to find the best similarity\n",
    "# threshold; set it as high as possible, avoiding incorrect substitutions in a gray area; and then\n",
    "# perform the replacement. It will avoid the repetition of original incorrect strings in the\n",
    "# output dataset, as well as wrong replacement (replacement by one of the standard strings which\n",
    "# is not the correct one).\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset.\n",
    "# The summary list is saved as summary_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "transf_dataset, summary_list = etl.string_replacement_ml (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, mode = MODE, threshold_for_percent_of_similarity = THRESHOLD_FOR_PERCENT_OF_SIMILARITY, list_of_dictionaries_with_standard_strings_for_replacement = LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **log-transforming the variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#### WARNING: This function will eliminate rows where the selected variables present \n",
    "#### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_log\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "log_transf_df = etl.log_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)\n",
    "\n",
    "# One curve derived from the normal is the log-normal.\n",
    "# If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "# A log normal curve resembles a normal, but with skewness (distortion); \n",
    "# and kurtosis (long-tail).\n",
    "\n",
    "# Applying the log is a methodology for normalizing the variables: \n",
    "# the sample space gets shrinkled after the transformation, making the data more \n",
    "# adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "# the transformation to the whole dataset, so that all variables will be of same order \n",
    "# of magnitude.\n",
    "# Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "# value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = etl.reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b9724a7d-15c6-4472-a0de-a592cda99b0a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining and applying Box-Cox transform**\n",
    "- Transform a series of data into a series described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "50e549c9-b761-44bc-bdb2-35ffabf9eab2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "MODE = 'calculate_and_apply'\n",
    "# Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "# transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "# To 'apply_only', lambda_box must be provided.\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_BoxCoxTransf'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "# identified as 'Y_BoxCoxTransf'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "SPECIFICATION_LIMITS = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# If there are specification limits, input them in this dictionary. Do not modify the keys,\n",
    "# simply substitute None by the lower and/or the upper specification.\n",
    "# e.g. Suppose you have a tank that cannot have more than 10 L. So:\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': 10}, there is only\n",
    "# an upper specification equals to 10 (do not add units);\n",
    "# Suppose a temperature cannot be lower than 10 ºC, but there is no upper specification. So,\n",
    "# specification_limits = {'lower_spec_lim': 10, 'upper_spec_lim': None}. Finally, suppose\n",
    "# a liquid which pH must be between 6.8 and 7.2:\n",
    "# specification_limits = {'lower_spec_lim': 6.8, 'upper_spec_lim': 7.2}\n",
    "\n",
    "#New dataframe saved as data_transformed_df; dictionary saved as data_sum_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "data_transformed_df, data_sum_dict = etl.box_cox_transform (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, mode = MODE, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX, specification_limits = SPECIFICATION_LIMITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "60059372-e5bc-4159-961d-e4b79ff0e889",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f6f1e23f-77d1-4194-b59c-2101fd604b93",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = etl.reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "72c3cf5f-dde2-4f3b-929c-681cbe228c1a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.For a category \"A\", a column named \"A\" is created.\n",
    "    - If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "    - If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "33e0b50c-10c3-46c0-877a-32be0c593d58",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as one_hot_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as OneHot_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, OneHot_encoding_list = etl.OneHotEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fc6b3659-fa74-4c86-8d41-9f20f4232152"
   },
   "source": [
    "### **Reversing the One-Hot Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9e3a3262-5ace-4825-af2c-5e9999847baa",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}}\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OneHotEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'column': string with the original column name (in quotes); \n",
    "# key 'OneHot_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'OneHot_enc_obj', storing the encoder object, only the key 'OneHot_enc_obj' is required.\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_columns': this key must store a list or array with the names of the columns\n",
    "# obtained from Encoding.\n",
    "\n",
    "# New dataframe saved as reversed_one_hot_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_one_hot_encoded_df = etl.reverse_OneHotEncode (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3cee6ad2-66f9-4504-8374-b04f3a818bdb"
   },
   "source": [
    "### **Ordinal Encoding the categorical variables**\n",
    "- Transform categorical values with notion of order into numerical (integer) features.\n",
    "- For each column, the Ordinal Encoder creates a new column in the dataset. This new column is represented by a an integer value, where each integer represents a possible categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4f8cdb7-01d0-4d10-b1ee-942ea95f38f3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as ordinal_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as ordinal_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "ordinal_encoded_df, ordinal_encoding_list = etl.OrdinalEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d3ed1009-13c7-4478-a582-0d795c107afc"
   },
   "source": [
    "### **Reversing the Ordinal Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6fcac64f-9dee-4f56-964f-637e777524a7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}}\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OrdinalEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'column': string with the original column name (in quotes); \n",
    "# key 'ordinal_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'ordinal_enc_obj', storing the encoder object, only the key 'ordinal_enc_obj' is required.\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_column': this key must store a string with the name of the column\n",
    "# obtained from Encoding.\n",
    "\n",
    "# New dataframe saved as reversed_ordinal_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_ordinal_encoded_df = etl.reverse_OrdinalEncode (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "89591bf7-2900-4ae0-9f77-0d020cdf9feb"
   },
   "source": [
    "### **Scaling the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d0a50802-8b35-4d0d-9d33-45b1e44afcd1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor', MODE = 'normalize_by_maximum'\n",
    "## This function provides 4 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "## MODE = 'normalize_by_maximum' is similar to MODE = 'factor', but the factor will be selected\n",
    "# as the maximum value. This mode is available only for SCALE_WITH_NEW_PARAMS = True. If\n",
    "# SCALE_WITH_NEW_PARAMS = False, you should provide the value of the maximum as a division 'factor'.\n",
    "\n",
    "SCALE_WITH_NEW_PARAMS = True\n",
    "# Alternatively, set SCALE_WITH_NEW_PARAMS = True if you want to calculate a new\n",
    "# scaler for the data; or SCALE_WITH_NEW_PARAMS = False if you want to apply \n",
    "# parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "# previously trained to another set of data; or wants to simply apply again the same\n",
    "# scaler).\n",
    "    \n",
    "## WARNING: The MODE 'factor' demmands the input of the list of factors that will be \n",
    "# used for normalizing each column. Therefore, it can be used only \n",
    "# when SCALE_WITH_NEW_PARAMS = False.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = None\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "# The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "# through vectorial operations.\n",
    "\n",
    "SUFFIX = '_scaled'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "# identified as 'Y_scaled'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as scaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "scaled_df, scaling_list = etl.feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, mode = MODE, scale_with_new_params = SCALE_WITH_NEW_PARAMS, list_of_scaling_params = LIST_OF_SCALING_PARAMS, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e5277523-9e36-4fa2-93c8-4cffab51204c"
   },
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9a19ae48-1669-4e22-a680-f7d9ccfb956d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = [\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}},\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}}\n",
    "                            \n",
    "                         ]\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as rescaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df, scaling_list = etl.reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, list_of_scaling_params = LIST_OF_SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "873bf804-9640-4cb2-92ad-b79f8602d265",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f575b478-8930-4400-bb78-36cec4853866"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a7f53fc3-8839-4e22-81e7-f7b3b638f52c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "82a45c25-3b08-4257-8466-2ba8c40087bd"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "70e950c4-eadb-43ea-93d0-f09c5dae61ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1716cb5a-0c72-4cf0-9207-051717b2df4b"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f63a5a18-0797-4c14-ab10-7aa77914f5d3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9a900908-dece-4106-89f3-5a979e50be3d"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "25dadb54-914a-451f-b7b4-85c2ae6beedb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "idsw.import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8d5503e7-4826-4e84-bbcb-ff5fd96f9d1a"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6246a5bf-49f4-4cba-9f3c-bf102403f659",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d7b63e08-99c4-47ba-a00b-d209faef3843"
   },
   "source": [
    "### **Renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "242ee25f-dabc-49dd-9a44-38527fe186e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'set_new_names'\n",
    "# MODE = 'set_new_names' will change the columns according to the specifications in\n",
    "# LIST_OF_COLUMNS_LABELS.\n",
    "\n",
    "# MODE = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "# upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "\n",
    "# MODE = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "# 'COLUMN' will be renamed as 'column'.\n",
    "\n",
    "# MODE = 'replace_substring' will search on the columns names (strings) for the \n",
    "# SUBSTRING_TO_BE_REPLACED (which may be a character or a string); and will replace it by \n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT (which again may be either a character or a string). \n",
    "# Numbers (integers or floats) will be automatically converted into strings.\n",
    "# As an example, consider the default situation where we search for a whitespace ' ' and replace it\n",
    "# by underscore '_': SUBSTRING_TO_BE_REPLACED = ' ', NEW_SUBSTRING_FOR_REPLACEMENT = '_'  \n",
    "# In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "\n",
    "# MODE = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "# e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "# 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "\n",
    "# MODE = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "# substring from the columns' names. \n",
    "# The substring must be indicated as TRAILING_SUBSTRING, and its default, when no value\n",
    "# is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "# e.g., if TRAILING_SUBSTRING = '_test' and you have a column named 'col_test', it will be \n",
    "# renamed as 'col'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = ' '\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = '_'\n",
    "\n",
    "TRAILING_SUBSTRING = None\n",
    "\n",
    "LIST_OF_COLUMNS_LABELS = [\n",
    "    \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_COLUMNS_LABELS = [{'column_name': None, 'new_column_name': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original column name; and the second one contains the new name\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "# by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "# The object LIST_OF_COLUMNS_LABELS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'column_name' for the original label; \n",
    "# and 'new_column_name', for the correspondent new label.\n",
    "# Notice that this function will not search substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "# labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "# where original_col and new_col represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "# Example: LIST_OF_COLUMNS_LABELS = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "# rename 'col1' as 'col'.\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = etl.rename_or_clean_columns_labels (df = DATASET, mode = MODE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, trailing_substring = TRAILING_SUBSTRING, list_of_columns_labels = LIST_OF_COLUMNS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8551a853-4b4a-4708-a471-a679711acb78"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b996a36b-9f62-46d5-82d4-d828ced2527e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = etl.df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a0bb249a-5d13-483d-adb4-b7f25e9b8ca4"
   },
   "source": [
    "### **Obtaining correlation plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9ade47a0-205c-485e-b0d5-ad010823d3bc",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'correlation_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = etl.correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ff763e69-c3d3-4029-b25e-c43789785967"
   },
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "22290618-2884-44ef-a588-d653076996f3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = etl.scatter_plot_lin_reg (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0dfc5f4a-0d7b-46c7-8671-960fa6936567"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "94265248-c714-454e-8fd8-2636a46220e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "etl.time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c9625bd5-c612-46de-9642-afd9be336ce6"
   },
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "TOTAL_OF_BINS = 10\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided.\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify these objects on the left of equality:\n",
    "general_stats, frequency_table = etl.histogram (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, total_of_bins = TOTAL_OF_BINS, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "acc57636-2d61-4aaf-8b97-ebb9bff6ebca"
   },
   "source": [
    "### **Testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "    \n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'probability_plot_normal.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = etl.test_data_normality (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "805a98d9-e0e9-4ebd-bb30-b2e13959b7bf"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "55b24b35-e9ae-437e-858f-aed34972f485",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "idsw.export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a6708648-6942-4da5-987a-a8ebb61fd537"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "db69901d-3841-4e8d-a60f-834537d58aa3"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f3cd0390-3343-4ad9-8001-f514e629b782",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e678c975-8acc-4bd3-9457-e9a66e8359b2"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c48fb828-a6d4-4efe-a517-539c383434ba",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bbb977f9-fb1e-4a1b-89e0-919914d64b9b"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a36fba56-6e01-402a-b2c8-455e5b046a95",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
