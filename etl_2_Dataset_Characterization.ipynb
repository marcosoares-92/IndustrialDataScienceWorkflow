{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "00750668-aee5-432d-a9c8-c3cb76951d6a"
   },
   "source": [
    "# **Dataset Characterization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "942bceb6-e596-462c-9ebc-39e8f01e1215"
   },
   "source": [
    "## _ETL Workflow Notebook 2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "83992759-51b4-4a5f-816c-ff79d50dccbf"
   },
   "source": [
    "## Content:\n",
    "1. Dataframe general characterization; \n",
    "2. Characterizing categorical variables;\n",
    "3. Removing all columns and rows that contain only missing values;\n",
    "4. Visualizing and characterizing distribution of missing values;\n",
    "5. Visualizing missingness across a variable, and comparing it to another variable (both numeric);\n",
    "6. Dealing with missing values; \n",
    "7. Obtaining the correlation plot;\n",
    "8. Plotting bar charts;\n",
    "9. Calculating cumulative statistics; \n",
    "10. Obtaining scatter plots and simple linear regressions;\n",
    "11. Performing the polynomial fitting; \n",
    "12. Visualizing time series; \n",
    "13. Visualizing histograms; \n",
    "14. Testing normality and visualizing the probability plot;\n",
    "15. Testing and visualizing probability plots for different statistical distributions;\n",
    "16. Filtering (selecting); ordering; or renaming columns from the dataframe;\n",
    "17. Renaming specific columns from the dataframe; or cleaning columns' labels;\n",
    "18. Dropping specific columns or rows from the dataframe; \n",
    "19. Removing duplicate rows from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "3f058d0c-6337-48e5-b0a0-f2ac1ab83d5a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import idsw\n",
    "from idsw import etl\n",
    "from idsw.etl import etl_workflow as ewf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "32230c8d-f1cf-420e-be2e-f8cd95114776",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f0a2be5e-53db-4bfe-9c50-d2fbaa7e8679",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c14a67f9-23ea-4179-8869-319af38cbbb7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b59ae45c-5573-426d-91ba-ec0b08d266a4",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "843eaff0-2bea-46bb-9437-5e53e4381bc1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ffe1bbda-3603-467c-b499-37c007fc70b8",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = idsw.json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "45380112-6b2f-4809-9ea3-770492c99ed5"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "81455331-34ae-4454-b9e7-738c19fc5fa2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = ewf.df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bc839c36-1ddf-4986-9f29-6de30274edbf"
   },
   "source": [
    "### **Characterizing the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0c0f5927-8bf4-46ec-9f75-7381c749dd64",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = None\n",
    "# TIMESTAMP_TAG_COLUMN: name (header) of the column containing the timestamps. \n",
    "# Keep TIMESTAMP_TAG_COLUMN = None if the dataframe do not contain timestamps.\n",
    "\n",
    "# Dataframe with summary from the categorical variables returned as cat_vars_summary. \n",
    "# Simply modify this object on the left of equality:\n",
    "cat_vars_summary = ewf.characterize_categorical_variables (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "88e4c93e-39e3-42b6-b85d-1971d456dade"
   },
   "source": [
    "### **Removing all columns and rows that contain only missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cdda68a5-dfbd-43a4-b373-b41e2dc6098d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_IGNORE = None\n",
    "# list_of_columns_to_ignore: if you do not want to check a specific column, pass its name\n",
    "# (header) as an element from this list. It should be declared as a list even if it contains\n",
    "# a single value.\n",
    "# e.g. list_of_columns_to_ignore = ['column1'] will not analyze missing values in column named\n",
    "# 'column1'; list_of_columns_to_ignore = ['col1', 'col2'] will ignore columns 'col1' and 'col2'\n",
    "\n",
    "# Cleaned dataframe returned as cleaned_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = ewf.remove_completely_blank_rows_and_columns (df = DATASET, list_of_columns_to_ignore = LIST_OF_COLUMNS_TO_IGNORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2e585a63-a745-4858-878c-5380756da2bd"
   },
   "source": [
    "### **Visualizing and characterizing distribution of missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9582b58c-86ba-4059-9679-414a7a145224",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SLICE_TIME_WINDOW_FROM = None\n",
    "SLICE_TIME_WINDOW_TO = None    \n",
    "# SLICE_TIME_WINDOW_FROM and SLICE_TIME_WINDOW_TO (timestamps). When analyzing time series,\n",
    "# use these parameters to observe only values in a given time range.\n",
    "    \n",
    "# SLICE_TIME_WINDOW_FROM: the inferior limit of the analyzed window. If you declare this value\n",
    "# and keep SLICE_TIME_WINDOW_TO = None, then you will analyze all values that comes after\n",
    "# SLICE_TIME_WINDOW_FROM.\n",
    "# SLICE_TIME_WINDOW_TO: the superior limit of the analyzed window. If you declare this value\n",
    "# and keep SLICE_TIME_WINDOW_FROM = None, then you will analyze all values until\n",
    "# SLICE_TIME_WINDOW_TO.\n",
    "# If SLICE_TIME_WINDOW_FROM = SLICE_TIME_WINDOW_TO = None, only the standard analysis with\n",
    "# the whole dataset will be performed. If both values are specified, then the specific time\n",
    "# window from 'SLICE_TIME_WINDOW_FROM' to 'SLICE_TIME_WINDOW_TO' will be analyzed.\n",
    "# e.g. SLICE_TIME_WINDOW_FROM = 'May-1976', and SLICE_TIME_WINDOW_TO = 'Jul-1976'\n",
    "# Notice that the timestamps must be declares in quotes, just as strings.\n",
    "\n",
    "AGGREGATE_TIME_IN_TERMS_OF = None    \n",
    "# AGGREGATE_TIME_IN_TERMS_OF = None. Keep it None if you do not want to aggregate the time\n",
    "# series. Alternatively, set AGGREGATE_TIME_IN_TERMS_OF = 'Y' or AGGREGATE_TIME_IN_TERMS_OF = \n",
    "# 'year' to aggregate the timestamps in years; set AGGREGATE_TIME_IN_TERMS_OF = 'M' or\n",
    "# 'month' to aggregate in terms of months; or set AGGREGATE_TIME_IN_TERMS_OF = 'D' or 'day'\n",
    "# to aggregate in terms of days.\n",
    "\n",
    "# Dataframes containing total of missing values and percent of missing values for each variable\n",
    "# returned as total_of_missing_values and percent_of_missing_values.\n",
    "# Simply modify these objects on the left of equality:\n",
    "df_missing_values = ewf.visualize_and_characterize_missing_values (df = DATASET, slice_time_window_from = SLICE_TIME_WINDOW_FROM, slice_time_window_to = SLICE_TIME_WINDOW_TO, aggregate_time_in_terms_of = AGGREGATE_TIME_IN_TERMS_OF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d424291b-e7ab-43b3-bfba-d64f267090ea"
   },
   "source": [
    "### **Visualizing missingness across a variable, and comparing it to another variable (both numeric)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a54cb498-b19c-43cc-a29e-c49c63ba1496",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column1'\n",
    "COLUMN_TO_COMPARE_WITH = 'column2'\n",
    "# COLUMN_TO_ANALYZE, COLUMN_TO_COMPARE_WITH: strings (in quotes).\n",
    "# COLUMN_TO_ANALYZE is the column from the dataframe df that will be analyzed in terms of\n",
    "# missingness; whereas COLUMN_TO_COMPARE_WITH is the column to which column_to_analyze will\n",
    "# be compared.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'column1' will analyze 'column1' from df.\n",
    "# COLUMN_TO_COMPARE_WITH = 'column2' will compare 'column1' against 'column2'\n",
    "\n",
    "SHOW_INTERPRETED_EXAMPLE = False\n",
    "# SHOW_INTERPRETED_EXAMPLE: set as True if you want to see an example of a graphic analyzed and\n",
    "# interpreted.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'comparison_of_missing_values.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "ewf.visualizing_and_comparing_missingness_across_numeric_vars (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_to_compare_with = COLUMN_TO_COMPARE_WITH, show_interpreted_example = SHOW_INTERPRETED_EXAMPLE, grid = GRID, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "899cf7bd-ad61-4278-8d96-eabedd83394b"
   },
   "source": [
    "### **Dealing with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9d584876-c022-4893-a584-201e4aae26ab",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be manipulated\n",
    "\n",
    "SUBSET_COLUMNS_LIST = None\n",
    "# SUBSET_COLUMNS_LIST = list of columns to look for missing values. Only missing values\n",
    "# in these columns will be considered for deciding which columns to remove.\n",
    "# Declare it as a list of strings inside quotes containing the columns' names to look at,\n",
    "# even if this list contains a single element. e.g. subset_columns_list = ['column1']\n",
    "# will check only 'column1'; whereas subset_columns_list = ['col1', 'col2', 'col3'] will\n",
    "# chek the columns named as 'col1', 'col2', and 'col3'.\n",
    "# ATTENTION: Subsets are considered only for dropping missing values, not for filling.\n",
    "    \n",
    "DROP_MISSING_VAL = True\n",
    "# DROP_MISSING_VAL = True to eliminate the rows containing missing values.\n",
    "# Alternatively: DROP_MISSING_VAL = False to use the filling method.\n",
    "\n",
    "FILL_MISSING_VAL = False\n",
    "# FILL_MISSING_VAL = False. Set this to True to activate the mode for filling the missing\n",
    "# values.\n",
    "\n",
    "ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False\n",
    "# ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False - This parameter shows effect only when\n",
    "# DROP_MISSING_VAL = True. If you set ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = True, then\n",
    "# only the rows where all the columns are missing will be eliminated.\n",
    "# If you define a subset, then only the rows where all the subset columns are missing\n",
    "# will be eliminated.\n",
    "\n",
    "MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT = None\n",
    "# This parameter shows effect only when DROP_MISSING_VAL = True. \n",
    "# If you set MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT equals to an integer value,\n",
    "# then only the rows where at least this integer number of non-missing values will be kept\n",
    "# after dropping the NAs.\n",
    "# e.g. if MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT = 2, only rows containing at\n",
    "# least two columns without missing values will be kept.\n",
    "# If you define a subset, then the criterium is applied only to the subset.\n",
    "\n",
    "VALUE_TO_FILL = None\n",
    "# VALUE_TO_FILL = None - This parameter shows effect only when\n",
    "# FILL_MISSING_VAL = True. Set this parameter as a float value to fill all missing\n",
    "# values with this value. e.g. VALUE_TO_FILL = 0 will fill all missing values with\n",
    "# the number 0. You can also pass a function call like \n",
    "# VALUE_TO_FILL = np.sum(dataset['col1']). In this case, the missing values will be\n",
    "# filled with the sum of the series dataset['col1']\n",
    "# Alternatively, you can also input a string to fill the missing values. e.g.\n",
    "# VALUE_TO_FILL = 'text' will fill all the missing values with the string \"text\".\n",
    "\n",
    "# You can also input a dictionary containing the column(s) to be filled as key(s);\n",
    "# and the values to fill as the correspondent values. For instance:\n",
    "# VALUE_TO_FILL = {'col1': 10} will fill only 'col1' with value 10.\n",
    "# VALUE_TO_FILL = {'col1': 0, 'col2': 'text'} will fill 'col1' with zeros; and will\n",
    "# fill 'col2' with the value 'text'\n",
    "\n",
    "FILL_METHOD = \"fill_with_zeros\"\n",
    "# FILL_METHOD = \"fill_with_zeros\". - This parameter shows effect only \n",
    "# when FILL_MISSING_VAL = True.\n",
    "# Alternatively: FILL_METHOD = \"fill_with_zeros\" - fill all the missing values with 0\n",
    "    \n",
    "# FILL_METHOD = \"fill_with_value_to_fill\" - fill the missing values with the value\n",
    "# defined as the parameter value_to_fill\n",
    "    \n",
    "# FILL_METHOD = \"fill_with_avg_or_mode\" - fill the missing values with the average value for \n",
    "# each column, if the column is numeric; or fill with the mode, if the column is categorical.\n",
    "# The mode is the most commonly observed value.\n",
    "    \n",
    "# FILL_METHOD = \"ffill\" - Forward (pad) fill: propagate last valid observation forward \n",
    "# to next valid.\n",
    "# FILL_METHOD = 'bfill' - backfill: use next valid observation to fill gap.\n",
    "# FILL_METHOD = 'nearest' - 'ffill' or 'bfill', depending if the point is closest to the\n",
    "# next or to the previous non-missing value.\n",
    "\n",
    "# FILL_METHOD = \"fill_by_interpolating\" - fill by interpolating the previous and the \n",
    "# following value. For categorical columns, it fills the\n",
    "# missing with the previous value, just as like FILL_METHOD = 'ffill'\n",
    "\n",
    "INTERPOLATION_ORDER = 'linear'\n",
    "# INTERPOLATION_ORDER: order of the polynomial used for interpolating if fill_method =\n",
    "# \"fill_by_interpolating\". If INTERPOLATION_ORDER = None, INTERPOLATION_ORDER = 'linear',\n",
    "# or INTERPOLATION_ORDER = 1, a linear (1st-order polynomial) will be used.\n",
    "# If INTERPOLATION_ORDER is an integer > 1, then it will represent the polynomial order.\n",
    "# e.g. INTERPOLATION_ORDER = 2, for a 2nd-order polynomial; INTERPOLATION_ORDER = 3 for a\n",
    "# 3rd-order, and so on.\n",
    "    \n",
    "# WARNING: if the fillna method is selected (FILL_MISSING_VAL == True), but no filling\n",
    "# methodology is selected, the missing values of the dataset will be filled with 0.\n",
    "# The same applies when a non-valid fill methodology is selected.\n",
    "# Pandas fillna method does not allow us to fill only a selected subset.\n",
    "# WARNING: if FILL_METHOD == \"fill_with_value_to_fill\" but value_to_fill is None, the \n",
    "# missing values will be filled with the value 0.\n",
    "\n",
    "\n",
    "# New dataframe saved as cleaned_df\n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = ewf.handle_missing_values (df = DATASET, subset_columns_list = SUBSET_COLUMNS_LIST, drop_missing_val = DROP_MISSING_VAL, fill_missing_val = FILL_MISSING_VAL, eliminate_only_completely_empty_rows = ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS, min_number_of_non_missing_val_for_a_row_to_be_kept = MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT, value_to_fill = VALUE_TO_FILL, fill_method = FILL_METHOD, interpolation_order = INTERPOLATION_ORDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "44ebe8c8-d008-45ab-837b-d45eaad0f78c"
   },
   "source": [
    "### **Advanced imputation on time series data: finding the best imputation strategy for missing values on a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e80bb137-877f-4b4f-af6b-1e2755a788aa",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function handles only one column by call, whereas handle_missing_values can process the whole\n",
    "# dataframe at once.\n",
    "# The strategies used for handling missing values is different here. You can use the function to\n",
    "# process data that does not come from time series, but only plot the graphs for time series data.  \n",
    "# This function is more indicated for dealing with missing values on time series data than handle_missing_values.\n",
    "# This function will search for the best imputer for a given column.\n",
    "# It can process both numerical and categorical columns.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be manipulated\n",
    "\n",
    "COLUMN_TO_FILL = None\n",
    "# string (in quotes) indicating the column with missing values to fill.\n",
    "# e.g. if COLUMN_TO_FILL = 'col1', imputations will be performed on column 'col1'.\n",
    "    \n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_tag_column\"\n",
    "# TIMESTAMP_TAG_COLUMN = None. string containing the name of the column with the timestamp. \n",
    "# If TIMESTAMP_TAG_COLUMN is None, the index will be used for testing different imputations.\n",
    "# be the time series reference. declare as a string under quotes. This is the column from \n",
    "# which we will extract the timestamps or values with temporal information. e.g.\n",
    "# TIMESTAMP_TAG_COLUMN = 'timestamp' will consider the column 'timestamp' a time column.\n",
    "\n",
    "TEST_VALUE_TO_FILL = None\n",
    "# TEST_VALUE_TO_FILL: the function will test the imputation of a constant. Specify this constant here\n",
    "# or the tested constant will be zero. e.g. TEST_VALUE_TO_FILL = None will test the imputation of 0.\n",
    "# TEST_VALUE_TO_FILL = 10 will test the imputation of value zero.\n",
    "\n",
    "SHOW_IMPUTATION_COMPARISON_PLOTS = True\n",
    "# SHOW_IMPUTATION_COMPARISON_PLOTS = True. Keep it True to plot the scatter plot comparison\n",
    "# between imputed and original values, as well as the Kernel density estimate (KDE) plot.\n",
    "# Alternatively, set SHOW_IMPUTATION_COMPARISON_PLOTS = False to omit the plots.\n",
    "\n",
    "# The following imputation techniques will be tested, and the best one will be automatically\n",
    "# selected: mean_imputer, median_imputer, mode_imputer, constant_imputer, linear_interpolation,\n",
    "# quadratic_interpolation, cubic_interpolation, nearest_interpolation, bfill_imputation,\n",
    "# ffill_imputation, knn_imputer, mice_imputer (MICE = Multiple Imputations by Chained Equations).\n",
    "    \n",
    "# MICE: Performs multiple regressions over random samples of the data; \n",
    "# Takes the average of multiple regression values; and imputes the missing feature value for the \n",
    "# data point.\n",
    "# KNN (K-Nearest Neighbor): Selects K nearest or similar data points using all the \n",
    "# non-missing features. It takes the average of the selected data points to fill in the missing \n",
    "# feature.\n",
    "# These are Machine Learning techniques to impute missing values.\n",
    "# KNN finds most similar points for imputing.\n",
    "# MICE performs multiple regression for imputing. MICE is a very robust model for imputation.\n",
    "\n",
    "\n",
    "# New dataframe saved as cleaned_df\n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = ewf.adv_imputation_missing_values (df = DATASET, column_to_fill = COLUMN_TO_FILL, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, test_value_to_fill = TEST_VALUE_TO_FILL, show_imputation_comparison_plots = SHOW_IMPUTATION_COMPARISON_PLOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5edf08b5-2760-4d81-bc01-410a510233eb"
   },
   "source": [
    "### **Applying a list of row filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a4fac3f4-9911-4884-9bf4-024f5ec907cd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = ewf.APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "531848c0-9a11-463a-a07f-512759894e42"
   },
   "source": [
    "### **Obtaining the correlation plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2a82adc9-f09f-43e7-b353-de1c0353fceb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'correlation_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = ewf.correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "952a3fd0-afbf-4572-92d9-b5cce42550eb"
   },
   "source": [
    "### **Plotting a bar chart**\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`.\n",
    "- For obtaining the **data distribution of categorical variables**, select any numeric column as the response, and set `aggregate_function = 'count'`. You can also set `plot_cumulative_percent = True` to compare the frequencies of each possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'bar_chart.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = ewf.bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a274c772-f4c2-4002-96f2-f88be33f215f"
   },
   "source": [
    "### **Calculating cumulative statistics**\n",
    "- Cumulative sum (cumsum); cumulative product (cumprod); cumulative maximum (cummax); cumulative minimum (cummin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "75c1a245-60a4-4cc2-94bd-0f31f716e4ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CUMULATIVE_STATISTIC = 'sum'\n",
    "# CUMULATIVE_STATISTIC: the statistic that will be calculated. The cumulative\n",
    "# statistics allowed are: 'sum' (for cumulative sum, cumsum); 'product' \n",
    "# (for cumulative product, cumprod); 'max' (for cumulative maximum, cummax);\n",
    "# and 'min' (for cumulative minimum, cummin).\n",
    "\n",
    "NEW_CUM_STATS_COL_NAME = None\n",
    "# NEW_CUM_STATS_COL_NAME = None or string (inside quotes), \n",
    "# containing the name of the new column created for storing the cumulative statistic\n",
    "# calculated. \n",
    "# e.g. NEW_CUM_STATS_COL_NAME = \"cum_stats\" will create a column named as 'cum_stats'.\n",
    "# If its None, the new column will be named as column_to_analyze + \"_\" + [selected\n",
    "# cumulative function] ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "\n",
    "# New dataframe saved as new_df\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = ewf.calculate_cumulative_stats (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, cumulative_statistic = CUMULATIVE_STATISTIC, new_cum_stats_col_name = NEW_CUM_STATS_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a49d1ed-08f7-4a57-9ef0-cb7f4cbb4f34"
   },
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = ewf.scatter_plot_lin_reg (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "802b8032-50f7-4227-86c9-9f7257a64240"
   },
   "source": [
    "### **Performing the polynomial fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1ea91e86-357a-4626-950f-2faf49bf50cb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "POLYNOMIAL_DEGREE = 6\n",
    "# Integer value representing the degree of the fitted polynomial.\n",
    "CALCULATE_ROOTS = False\n",
    "# CALCULATE_ROOTS = False.  Alternatively, set as True to calculate the roots of the\n",
    "#  fitted polynomial and return them as a NumPy array.\n",
    "CALCULATE_DERIVATIVE = False\n",
    "# CALCULATE_DERIVATIVE = False. Alternatively, set as True to calculate the derivative of the\n",
    "#  fitted polynomial and add it as a column of the dataframe.\n",
    "CALCULATE_INTEGRAL = False\n",
    "# CALCULATE_INTEGRAL = False. Alternatively, set as True to calculate the integral of the\n",
    "#  fitted polynomial and add it as a column of the dataframe.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "SHOW_POLYNOMIAL_REG = True\n",
    "#Alternatively: set SHOW_POLYNOMIAL_REG = True to plot the polynomial regressions graphics\n",
    "# calculated for each pair Y x X. \n",
    "# Set SHOW_LINEAR_REG = False to omit these plots.\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'polynomial_fitting.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = ewf.polynomial_fit (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, polynomial_degree = POLYNOMIAL_DEGREE, calculate_roots = CALCULATE_ROOTS, calculate_derivative = CALCULATE_DERIVATIVE, calculate_integral = CALCULATE_INTEGRAL, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_polynomial_reg = SHOW_POLYNOMIAL_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5b2a1a03-7736-45c9-8d65-5cf501159fa4"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "ewf.time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fb1c468c-2ab7-46d1-8866-88764537d748"
   },
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "15d28d6f-b8d5-4554-98af-3f41bf4f7ea0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "TOTAL_OF_BINS = 10\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided.\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify these objects on the left of equality:\n",
    "general_stats, frequency_table = ewf.histogram (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, total_of_bins = TOTAL_OF_BINS, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8c7f1908-6535-4326-84ad-b5dcbf456e93",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b4bb8f51-4265-4c84-afce-c6c33528e28a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "    \n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'probability_plot_normal.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = ewf.test_data_normality (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f756b5b4-2120-4b2a-bdff-0b21aac0414b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing and visualizing probability plots for different statistical distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "# Attention: if you want to test a normal distribution, use the function \n",
    "# test_data_normality.Function test_data_normality tests normality through 4 methods \n",
    "# and compare them: D’Agostino and Pearson’s; Shapiro-Wilk; Lilliefors; and Anderson-Darling tests.\n",
    "# The calculus of the p-value from the Anderson-Darling statistic is available only \n",
    "# for some distributions. The function specific for the normality calculates these \n",
    "# probabilities of following the normal.\n",
    "# Here, the function is destined to test a variety of distributions, and so only the \n",
    "# Anderson-Darling test is performed.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "\n",
    "STATISTICAL_DISTRIBUTION_TO_TEST = 'lognormal'\n",
    "#STATISTICAL_DISTRIBUTION: string (inside quotes) containing the tested statistical \n",
    "# distribution. \n",
    "## Notice: if data Y follow a 'lognormal', log(Y) follow a normal\n",
    "## Poisson is a special case from 'gamma' distribution.\n",
    "## There are 91 accepted statistical distributions:\n",
    "# 'alpha', 'anglit', 'arcsine', 'beta', 'beta_prime', 'bradford', 'burr', 'burr12', 'cauchy',\n",
    "# 'chi', 'chi-squared', 'cosine', 'double_gamma', 'double_weibull', \n",
    "# 'erlang', 'exponential', 'exponentiated_weibull', 'exponential_power',\n",
    "# 'fatigue_life_birnbaum-saunders', 'fisk_log_logistic', 'folded_cauchy', 'folded_normal',\n",
    "# 'F', 'gamma', 'generalized_logistic', 'generalized_pareto', 'generalized_exponential', \n",
    "# 'generalized_extreme_value', 'generalized_gamma', 'generalized_half-logistic', \n",
    "# 'generalized_inverse_gaussian', 'generalized_normal', \n",
    "# 'gilbrat', 'gompertz_truncated_gumbel', 'gumbel', 'gumbel_left-skewed', 'half-cauchy', \n",
    "# 'half-normal', 'half-logistic', 'hyperbolic_secant', 'gauss_hypergeometric', \n",
    "# 'inverted_gamma', 'inverse_normal', 'inverted_weibull', 'johnson_SB', 'johnson_SU', \n",
    "# 'KSone','KStwobign', 'laplace', 'left-skewed_levy', \n",
    "# 'levy', 'logistic', 'log_laplace', 'log_gamma', 'lognormal', 'log-uniform', 'maxwell', \n",
    "# 'mielke_Beta-Kappa', 'nakagami', 'noncentral_chi-squared', 'noncentral_F', \n",
    "# 'noncentral_t', 'normal', 'normal_inverse_gaussian', 'pareto', 'lomax', 'power_lognormal',\n",
    "# 'power_normal', 'power-function', 'R', 'rayleigh', 'rice', 'reciprocal_inverse_gaussian', \n",
    "# 'semicircular', 'student-t', 'triangular', \n",
    "# 'truncated_exponential', 'truncated_normal', 'tukey-lambda', 'uniform', 'von_mises', \n",
    "# 'wald', 'weibull_maximum_extreme_value', 'weibull_minimum_extreme_value', 'wrapped_cauchy'\n",
    "\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = ewf.test_stat_distribution (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, statistical_distribution_to_test = STATISTICAL_DISTRIBUTION_TO_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b852a4e3-ba17-44fb-b228-900b98cb1479"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "199f0f18-b8b3-423d-b163-d3b74a9c8811",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = ewf.select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0919250e-0203-4417-96df-964a14919bba"
   },
   "source": [
    "### **Renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b6f6da55-58b6-4f94-9fee-26f6622f70dd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'set_new_names'\n",
    "# MODE = 'set_new_names' will change the columns according to the specifications in\n",
    "# LIST_OF_COLUMNS_LABELS.\n",
    "\n",
    "# MODE = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "# upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "\n",
    "# MODE = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "# 'COLUMN' will be renamed as 'column'.\n",
    "\n",
    "# MODE = 'replace_substring' will search on the columns names (strings) for the \n",
    "# SUBSTRING_TO_BE_REPLACED (which may be a character or a string); and will replace it by \n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT (which again may be either a character or a string). \n",
    "# Numbers (integers or floats) will be automatically converted into strings.\n",
    "# As an example, consider the default situation where we search for a whitespace ' ' and replace it\n",
    "# by underscore '_': SUBSTRING_TO_BE_REPLACED = ' ', NEW_SUBSTRING_FOR_REPLACEMENT = '_'  \n",
    "# In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "\n",
    "# MODE = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "# e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "# 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "\n",
    "# MODE = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "# substring from the columns' names. \n",
    "# The substring must be indicated as TRAILING_SUBSTRING, and its default, when no value\n",
    "# is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "# e.g., if TRAILING_SUBSTRING = '_test' and you have a column named 'col_test', it will be \n",
    "# renamed as 'col'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = ' '\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = '_'\n",
    "\n",
    "TRAILING_SUBSTRING = None\n",
    "\n",
    "LIST_OF_COLUMNS_LABELS = [\n",
    "    \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_COLUMNS_LABELS = [{'column_name': None, 'new_column_name': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original column name; and the second one contains the new name\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "# by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "# The object LIST_OF_COLUMNS_LABELS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'column_name' for the original label; \n",
    "# and 'new_column_name', for the correspondent new label.\n",
    "# Notice that this function will not search substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "# labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "# where original_col and new_col represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "# Example: LIST_OF_COLUMNS_LABELS = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "# rename 'col1' as 'col'.\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = ewf.rename_or_clean_columns_labels (df = DATASET, mode = MODE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, trailing_substring = TRAILING_SUBSTRING, list_of_columns_labels = LIST_OF_COLUMNS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "824ec564-8540-4184-9a66-3ca8daf4892c"
   },
   "source": [
    "### **Merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5777fe37-4dab-41ef-bbdc-8b5077bb4741",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"DATE\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"DATE\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\". This option has no effect \n",
    "# if MERGE_METHOD = \"asof\". Keep inside quotes.\n",
    "\n",
    "MERGE_METHOD = \"asof\"\n",
    "# Alternatively: MERGE_METHOD = 'ordered' to use pandas .merge_ordered method, or\n",
    "# MERGE_METHOD = \"asof\" for using the .merge_asof method.\n",
    "# WARNING: .merge_asof uses fuzzy matching, so the HOW_TO_JOIN parameter is not applicable.\n",
    "# Keep inside quotes.\n",
    "\n",
    "## USE MERGE_METHOD = 'asof' to merge data collected asynchronously, i.e., data collected in\n",
    "# different moments, resulting in timestamps that do not perfectly match.\n",
    "# merge_asof method sorts the timestamps in ascending order and does not look for a perfect \n",
    "# combination of keys. Instead, it takes the timestamp from the right dataframe as key, and \n",
    "# searches for the closest dataframe on the left dataframe. So, it inputs the row from the right on \n",
    "# the correct position it should have in the left dataframe (in other words, it appends the rows from\n",
    "# one into the order, respecting the columns and the time order).\n",
    "# If a missing value would be generated, the 'ffill' parameter can be used to automatically \n",
    "# repeat the previous value (from the left dataframe) on the row that came from the right table,\n",
    "# filling the missing values.\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "ASOF_DIRECTION = \"nearest\"\n",
    "# Parameter of .merge_asof method. 'nearest' merge the closest timestamps in both directions.\n",
    "# Alternatively: 'backward' or 'forward'.\n",
    "# This option has no effect if MERGE_METHOD = \"ordered\". Keep inside quotes.\n",
    "\n",
    "ORDERED_FILLING = 'ffill'\n",
    "# Parameter or .merge_ordered method.\n",
    "# Alternatively: ORDERED_FILLING = 'ffill' (inside quotes) to fill missings \n",
    "# with the previous value.\n",
    "# This option has no effect if MERGE_METHOD = \"asof\", so you can keep it None\n",
    "\n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = ewf.MERGE_ON_TIMESTAMP (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merge_method = MERGE_METHOD, merged_suffixes = MERGED_SUFFIXES, asof_direction = ASOF_DIRECTION, ordered_filling = ORDERED_FILLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "86991fc9-144f-425e-a1f2-be16f08072cb"
   },
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cb774677-4355-4f48-9d6b-2cf06e180349",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = ewf.MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8906ee22-5bad-4916-8a35-cbb03595a25c"
   },
   "source": [
    "### **Dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "17bedbfc-0a0e-4c88-bdbe-519bdaaab057",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "WHAT_TO_DROP = 'columns'\n",
    "# WHAT_TO_DROP = 'columns' for removing the columns specified by their names (headers)\n",
    "# in COLS_LIST (a list of strings).\n",
    "# WHAT_TO_DROP = 'rows' for removing the rows specified by their indices in\n",
    "# ROW_INDEX_LIST (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "# the first row is row number zero.\n",
    "\n",
    "COLS_LIST = None\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to be removed\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "# If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "# e.g. COLS_LIST = 'col1'; or COLS_LIST = ['col1']\n",
    "\n",
    "ROW_INDEX_LIST = None\n",
    "# ROW_INDEX_LIST = a list of integers containing the indices of the rows that will be dropped.\n",
    "# e.g. ROW_INDEX_LIST = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "# 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "# a list).\n",
    "# e.g. ROW_INDEX_LIST = 20 or ROW_INDEX_LIST = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = ewf.drop_columns_or_rows (df = DATASET, what_to_drop = WHAT_TO_DROP, cols_list = COLS_LIST, row_index_list = ROW_INDEX_LIST, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "67295ed2-b106-4e24-8d3d-fcc4921fb1d7"
   },
   "source": [
    "### **Removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6636361d-82de-426a-8488-d1a3b601a4db",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_ANALYZE = None\n",
    "# if LIST_OF_COLUMNS_TO_ANALYZE = None, the whole dataset will be analyzed, i.e., rows\n",
    "# will be removed only if they have same values for all columns from the dataset.\n",
    "# Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "# same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "# being declared.\n",
    "# e.g. LIST_OF_COLUMNS_TO_ANALYZE = ['column1'] will check only 'column1'. Entries with same value\n",
    "# on 'column1' will be considered duplicates and will be removed.\n",
    "# LIST_OF_COLUMNS_TO_ANALYZE = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "# 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "# duplicates and will be removed.\n",
    "\n",
    "WHICH_ROW_TO_KEEP = 'first'\n",
    "# WHICH_ROW_TO_KEEP = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "# None or an invalid string is input, this method will be selected.\n",
    "# WHICH_ROW_TO_KEEP = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = ewf.remove_duplicate_rows (df = DATASET, list_of_columns_to_analyze = LIST_OF_COLUMNS_TO_ANALYZE, which_row_to_keep = WHICH_ROW_TO_KEEP, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "52556b7f-1d7b-454e-8f21-f842ca5c9b9c"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a13ad627-2b47-490e-96af-a7abb8591698",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "idsw.export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9a4ada30-ab24-414b-aefa-1e0d34ae950b"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "24cc13bf-cf6b-48e0-b156-25fffa27eacd"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1ef010b4-178f-4328-ab69-07b654b525e3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cc7aa510-bb3b-4473-9da7-ccc252ab257d"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4721ef19-6f7a-4b9f-ba9e-dd14064ac9f7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "idsw.upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ea1239cd-f63a-405e-ad33-a57610203de3"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9e67c15d-6990-48de-b8ed-cf941bb57921",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "idsw.export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
